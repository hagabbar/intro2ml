{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Method 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "MNIST is a handwrriten digit database with a training set of 60,000 examples and a test se of 10,000 examples. The images are all 28x28 and greyscale.\n",
    "\n",
    "It is available [here](http://yann.lecun.com/exdb/mnist/), but is included in Keras and will automatically download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the 'x' arrays are the images and the 'y' arrays are the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAADuCAYAAACnM7W+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGENJREFUeJzt3XmwFNUVx/HvAxVBZZWAJsWmURYFRVAkFGDCoogiElAD6jMGKIkoVmJi1BAICi6JFQQBI1FcqEITIohKkATwGQQjJKQKERdMUERkURYBHxEmf1jndg9vm3lvenpm7u/zD033nZn76HmX093nnluUSCQQEfFVrbg7ICISJw2CIuI1DYIi4jUNgiLiNQ2CIuI1DYIi4jUNgiLiNQ2CIuI1DYIi4rVj0mlcVFTk+/SSnYlEomncncg0nVed1wKV0nlVJJiezXF3QCKh81qYUjqvGgRFxGsaBEXEaxoERcRrGgRFxGsaBEXEaxoERcRrGgRFxGtpJUuLROm8885z2zfffDMA1113HQBPPfUUANOmTXNt/vnPf2axd1KoFAmKiNeK0lloKZvTcGrXru22GzRoUGE7ixjq1asHwJlnngnAj3/8Y9fmN7/5DQDXXHMNAF9++aU7dt999wEwceLEVLq1NpFIdEmlYT6Je3rVOeecA8CyZcvcvvr165fbds+ePW67SZMmmeqCzmsO+d73vgfA3LlzAejVq5c79s4776TzVimdV0WCIuI1DYIi4rVYHoy0aNHCbR933HEAdO/eHYAePXoA0LBhQ9dmyJAhKb/3li1bAHj44YfdvsGDBwOwb98+AP7973+7Y6+++mpafZfMOf/88wGYP38+kHzbw27T2Dk7dOgQkHwJ3K1bNyB4QGJtJHU9e/YEkv9dn3/++bi6A0DXrl0BePPNN7PyeYoERcRrWY0Ey7sBXtlDj3QcOXIEgLvvvhuAL774wh2zG6yffPIJAJ9//rk7luaNVqkme3DVuXNnt++ZZ54B4JRTTqnwde+99x4ADzzwAADz5s1zx1auXAkE53zKlCkZ7LEfevfuDcC3v/1tty+OSLBWrSAea926NQAtW7YEoKioKNrPjvTdRURyXFYjwQ8//BCAXbt2uX3pRIJvvPEGALt373b7LrroIiC4H/T000/XuJ+SeY8++igQpCmlyiLHE088EUi+h2tRTMeOHTPQQz9ZMvqqVati7Uf4amDkyJFAcKWwcePGSD9bkaCIeE2DoIh4LauXw5999hkAt99+u9s3cOBAAP71r38ByaktZt26dQD07dsXgP3797tjHTp0AODWW2+NoMdSUzYf+NJLLwXKv8ltl7iLFi1y+2yWz9atW4Hg+xF+qPXd7363wveU1IQfSMRp9uzZZfbZQ7Go5ca/gIhITGJJll6wYIHbtnQZS4rt1KkTADfeeKNrY1FBOAI0b731FgCjRo2KprNSLZYOtXTpUiCYCxyeq7548WIgeFgSniNqaS8WIezYsQNITnS3tCiLMsPpN6owUzl7mNSsWbOYe/K18h6Q2ncnaooERcRrsdcT3Lt3b9Lfw1VCjD0yf/bZZ4EgApDccsYZZ7htu+9r/8Pv3LkTCBLWAZ588kkgSGx/6aWX3LHwdlXq1q0LwE9+8hO3b/jw4Wn13TcDBgwAgn+7uFgkagnSYR9//HFW+qBIUES8FnskeLQJEyYAyVWG7V5Rnz59AHjllVey3i+pWJ06dYDg3i0EkYbd67Wk3DVr1rg2mY5CwoU5pHJWd9PYvfVss+9M+N7ku+++CwTfnagpEhQRr2kQFBGv5dzlsKXB2MMQCNIdHnvsMQCWL1/ujtnl1SOPPAIkp2BIdpx77rlAcAkcNmjQIEB1G3NdlLX7wkslXHzxxQCMGDECgH79+pVpP2nSJCC5RkCUFAmKiNdyLhI0mzZtctvFxcUAPPHEEwBce+217phtn3DCCUCwNGM4FUOi9dBDDwHJ09cs8osyArQpX0qZqrnGjRun1M4mM9i5toeV3/rWt1wbqxZvaUrhqXkHDx4EgopQpaWlABxzTDAUrV27Nv0foAYUCYqI13I2EgyzSrc2odoiDwiW55s8eTIQVKO99957XZtsJV36xopf2BS58P3YF154IfLPtwjQPtcKbUjVLCKzf7tZs2a5Y3feeWeFr7PpdhYJfvXVVwAcOHDAtdmwYQMAjz/+OJCcFmVXBp9++ikQrAkUTpeKun7g0RQJiojXNAiKiNfy4nLYrF+/HoBhw4a5fZdddhkQPDQZPXo0kLxwjNUhlMyySxi7Eb59+3Z3zOZ5Z4rNSrEZRWFWiegXv/hFRj+zkI0ZMwaAzZs3A8GSt1WxJTKsEtTbb78NwOrVq9P6fKv61LRpUwA++OCDtF6fSYoERcRreRUJmnASpS2sZHXn7FG7LSoNwYI8K1asyE4HPWXpDpC5FCWLAK2+YLgqud1U/+1vfwskL7Mqqbn//vtj+Vx7oGnmz58fSz9AkaCIeC6vIkF7PP/973/f7evatSuQnGwJwWN6gJKSkiz0TjKZFmNpNxb5XXXVVQAsXLjQtRkyZEjGPk/iFceC70aRoIh4LWcjwXC9s5tvvhmAK6+8EoDmzZtX+LrDhw8DyfekNK0qGpYwa39eccUV7lh1Vv+77bbb3PYvf/lLIKhMPXfuXCCoSyiSKYoERcRrGgRFxGs5czlsl7i2/KJdAgO0atWqytfb/ESbM5yNuau+s3mn9mf4NsXDDz8MBPNHd+3aBUC3bt1cG6sAZJVJwpVILCl3yZIlAMyYMSPzP4DEzm6lhBfpSjfxuqYUCYqI12KJBMOLqrRv3x6A6dOnA9C2bdsqX2+1yAAefPBBIEid0EOQ+NSuXdtt27QsS2OxpVXD0xmP9vrrr7ttqx4+fvz4jPdTcoddRYRrDmabIkER8VpWIkGrWvvoo48CQSIsQJs2bap8vUUINj3K7hNBUBdNsm/VqlVAsD6FJa6H2X3CcPRv7D7hvHnzgOql1UhhuPDCC932nDlzsvrZigRFxGsaBEXEaxm/HL7ggguA5Gof559/PgDf/OY3q3y9lem2FAsISufbcpySG6yKi83ksVqOEFR9OdrUqVPd9syZMwF4//33o+qi5Ljw4lxxUSQoIl7LeCQ4ePDgpD/LE67w8uKLLwLBgi328CNbCy9Lzdk87XDV5/IqQIuYxYsXAzB06NCYe6JIUEQ8VxReJrHKxkVFqTcuTGsTiUSXuDuRaTqvOq8FKqXzqkhQRLymQVBEvKZBUES8pkFQRLymQVBEvKZBUES8lm6y9E5gcxQdyRMt4+5ARHReC5POawrSyhMUESk0uhwWEa9pEBQRr2kQFBGvaRAUEa9pEBQRr2kQFBGvaRAUEa9pEBQRr2kQFBGvaRAUEa9pEBQRr6VVQEFrFrAzkUg0jbsTmabzqvNaoFI6r4oE0+NzRY5CpvNamFI6rxoERcRrGgRFxGsaBEXEaxoERcRrGgRFxGsaBEXEaxoERcRrGgRFxGsaBEXEa+muO5zz7r77bgAmTpzo9tWq9fVY37t3bwBeffXVrPdLxFcnnXSS2z7xxBMBuPTSSwFo2vTrWW0PPfSQa1NaWprF3ikSFBHPaRAUEa8VzOVwcXExAD//+c8BOHLkSJk2iYTvRTVEoteqVSsg+F288MIL3bGzzjqr3NeccsopbvuWW26JrnPlUCQoIl4rmEiwZcuWABx//PEx90Qqc8EFF7jtESNGANCrVy8AOnToUKb9T3/6UwC2bt0KQI8ePdyxZ555BoA33ngjms5Kldq2bQvAuHHj3L7hw4cDULduXQCKiorcsY8++giAffv2AdCuXTsAhg0b5trMmDEDgI0bN0bV7SSKBEXEa3kfCfbp0weAsWPHJu0P/y8ycOBAAD799NPsdUySXHXVVQBMnTrV7Tv55JOBIFJYsWKFO2apEw8++GDS+4SjCmtz9dVXZ77DUq4GDRoAcP/99wPBeQ2nwRztvffec9v9+/cH4NhjjwWC31P7Lhy9nQ2KBEXEaxoERcRreXk5HL45/sQTTwBBmG7Cl1GbN2sJiWw75pivv1pdunQB4LHHHgOgXr16rk1JSQkAkyZNAuDvf/+7O1anTh0AnnvuOQD69etX5jPWrFmT6W5LFQYPHgzAj370oyrbbtq0CYC+ffu6ffZg5PTTT4+gd9WjSFBEvJaXkeD111/vtk899dSkY3Zz/amnnspml+Qolv4ye/bspP1Lly5123ZTfe/evWVeb8eOjgC3bNnitp988snMdFZSNnTo0HL3//e//3Xbb775JhAkS1v0F2apMblAkaCIeC2vIkF7dP7DH/7Q7bPpcbt37wbgnnvuyX7HBAju7QHceeedQDBV0RJgrcoPlB8Bmrvuuqvc/eEpVTt27Kh+Z6VaRo4cCcCoUaMAeOWVVwB4//33XZvt27dX+T7NmjWLoHfVo0hQRLyWF5GgTcieP39+hW2mTZsGwPLly7PRJQkZP348EER/AIcOHQJgyZIlQHB/6ODBg2Veb1Mdw/f/WrRoAQTJ0RbhL1y4MKN9l/TY9MUJEybU6H3CRRXipkhQRLymQVBEvJYXl8MXX3wxAB07dixz7G9/+xuQPCdVsqNhw4YAjBkzBkiu12iXwVdccUWFr7eE2blz5wJw3nnnlWnzpz/9CYAHHnggAz2WbLCHVyeccEKFbc4+++ykv7/++utue9WqVdF0rAKKBEXEazkbCYYjiPvuuy/pWHh6lSVO79mzJzsdE+e4444Dyq/6YdHAN77xDQBuuOEGAC6//HLXxqoM2+I74UjStq1m4P79+zPad6kZm/7Yvn17AH71q1+5YwMGDEhqawudQdmK7/agxb4fAIcPH85sZ6ugSFBEvJZzkWAq6TAffPCB21aNwPhYGowlLVt9P4D//Oc/QOXrulgUYEnT4XUmdu7cCcCiRYsy2GOpDqv9B3DuuecCwe+nnbNw6pOdV7u3Z/f0IbmABgSFNq688kq3z+7v2/craooERcRrGgRFxGs5dzlc2ZKZ5ugHJRIPm69tD7FefPFFd6xx48ZAUFPOZnrMmTPHtfnss88AmDdvHpB8OWz7JD724Ct8OfvnP/85qc3EiRMBWLZsmdu3cuVKIPgOhI8dveSm3UKZMmWK2/fhhx8CsGDBAgBKS0tr8FNUTZGgiHgtZyLBc845Byi/grCxaOKdd97JSp8kNbbkZfjBSCp69uwJBEtuhqP/8MMvyS57EGJR3u23316mzeLFi4Fgzr5dFUDwPXj55ZeB5MRoe9hhye8WGQ4aNMi1seT5v/71r0CwqBPA559/ntSPdevWpfGTlU+RoIh4LWciQatL1qhRozLHVq9eDUBxcXE2uyQRs8W5LQIMp9PonmB21a5d221bXUhb+D6cqH7HHXcAwfmxCNDWkgGYPn06EKTThJfcvOmmm4Cg2lP9+vUB6N69u2tji7dbYn24GrmxatWtW7dO+WesiCJBEfFazkSCTZo0Acp/KmxVib/44ous9kmiZUUWJH5WKRqCCPDAgQMAjB492h2zK7Zu3boBwXS3Sy65xLWxCP/Xv/41EKwICWXXG7FE+b/85S9un21fc801APzgBz8o09/bbrstxZ+saooERcRrGgRFxGtFlc3tLNO4qCj1ximyUNkeepR3OdymTRsgJxZRX5tIJLpU3Sy/RHFeU9G/f38gSKUIfxctcTpLiyl5f14/+eQTt20pLpakvHHjRnfMagRWtni6ld63BOhsV4UJSem8KhIUEa/F8mDEEqMB+vTpAwQRoCVTPvLII66NKsUUJovwJX7btm1z2xYJ1qlTB4BOnTqVaW/Re0lJCRBMcYNgIfYYI8C0KBIUEa/FEgna2hQAzZs3Tzr28ccfA8Fjeilcr732GhBUHq6saIZEy6YwQlAQo3PnzkDyYuqPP/44EExfy1bNvygpEhQRr2kQFBGv5cyMEfHP+vXrgWBuafhByWmnnQZkLUXGe/v27XPbTz/9dNKfhU6RoIh4LZZIMJx8aYsu9+jRI46uSA6YPHkyALNnz3b77r33XgDGjh0LwIYNG7LfMfGCIkER8Vrs0+byjPfTq6JgNeWee+45t8+S6G1NC6tWEtEi7DqvhUnT5kREqqJIMD2KGCJkESEE9wStEnHHjh2ByO4N6rwWJkWCIiJV0SAoIl7T5XB6dNlUmHReC5Muh0VEqpJusvROIPbyzjFqGXcHIqLzWph0XlOQ1uWwiEih0eWwiHhNg6CIeE2DoIh4TYOgiHhNg6CIeE2DoIh4TYOgiHhNg6CIeE2DoIh4TYOgiHgtrbnDqkrBzkQi0TTuTmSazqvOa4FK6bwqEkyPz5PRC5nOa2FK6bxqEBQRr2kQFBGvaRAUEa9pEBQRr2kQFBGvaRAUEa9pEBQRr6W70FLWTJ061W3fcsstAKxfvx6AgQMHumObNyvFS0SqT5GgiHgt5yLBVq1aATBixAi378iRIwC0a9cOgLZt27pjigTzwxlnnAHAscce6/b17NkTgBkzZgDBeU7VwoULAbj66qsBOHToUI37KdUTPq/du3cHYPLkyQB85zvfiaVPqVIkKCJe0yAoIl7LucvhHTt2AFBSUuL2XX755XF1R6qpQ4cOABQXFwMwdOhQAGrVCv7fPfXUU4HgMjiRSK/oiX0vZs2aBcC4cePcsb1791aj11JdDRo0cNvLly8HYNu2bQA0b97cHbN9uUSRoIh4Leciwf379wN64JHvpkyZAsCAAQMi/6zrrrsOgD/84Q9u38qVKyP/XKmcRYCKBEVEcljORYINGzYEoFOnTjH3RGpi6dKlQNlIcPv27W7bIje7T1heioylW/Tq1SuSfkp0ioqK4u5CShQJiojXNAiKiNdy7nK4Xr16ALRo0aLCNl27dnXbGzduBPQgJdfMnDkTgAULFiTt/9///ue2U7lJXr9+fSCYN25pNWH2GWvWrKleZyUSlvJ0/PHHx9yTyikSFBGv5VwkuHXrVgDmzJnj9k2YMCGpTfjvu3fvBmD69OlRd03S8NVXXwHw0Ucf1eh9+vfvD0CjRo0qbLNlyxYASktLa/RZEo0uXbq47dWrV8fYk/IpEhQRr+VcJGgmTZrkto+OBKXwWWWYkSNHAlC3bt0K244fPz4rfZKKWeQPsGfPHiCYSnfaaafF0qdUKRIUEa/lbCQYVlkyreS/4cOHA3DHHXe4faeffjqQXKfuaOvWrQOSnzhLPOzePMBrr70GJFeAz2WKBEXEaxoERcRreXE5XN16cxIfWybh2muvBaBPnz4Vtu3RowdQ+fm1+oDhS+aXX34ZgIMHD9aor+I3RYIi4rW8iAQlP5x11llu+4UXXgAqn/6YDrvZ/vvf/z4j7yfZ06RJk7i7UClFgiLiNUWCEgmrJZdKTblUUqAs3eKSSy5x+xYvXlyTLkqW5PoaQYoERcRrGgRFxGt5cTlc2eVSz549AVWRyQVW8w+gd+/eAIwYMQKAJUuWAPDll1+m9F433ngjAGPHjs1gDyUbbMlNzRgREckDRekkIBcVFcWSrXz48GGg8mTajh07ArBhw4You7I2kUh0qbpZfonrvFbGKpDs2rUraf9ll13mtjP4YETnNYOGDBkCwB//+EcgOZm9ffv2QNYqwad0XhUJiojX8uKe4KxZswAYPXp0hW1GjRoFwLhx47LSJ4mWVZSW/BOuLQjJaVJ16tTJdneqpEhQRLyWF5GgrSgnucVq/fXr1w+AZcuWuWPVKWpwww03uO2pU6fWsHcSl4ULFwLB723btm3dMbtSGzNmTPY7VgFFgiLiNQ2CIuK1vEiRMe+++y5Q/sItllBtZdk3bdoURRe8T6Ww2n8Ad911FwB9+/YFoHXr1u5YKkttNm7cGIABAwYAMG3aNHfspJNOSmprl9fheaiWlJsB3p/XKPzud78Dkm9zNGvWDEg9ab6GlCIjIlKVvHgwYt566y0A2rRpU+aYFmHKjvD0xHD9QICf/exnbnvfvn1VvpdFkJ07dwbKT4ZfsWIFADNnzgQyGv1JloTP66FDh2LsSfkUCYqI1/IqErSqwuGpU5I7brrpphq9fvv27W570aJFANx6661A1u4hSQTq16/vtgcNGgTA888/H1d3ylAkKCJe0yAoIl7Lq8thqxDz9ttvu33t2rWLqzteKi4udttW6+/6669P+fXh1KUDBw4A5S+iFK5NKPlp2LBhAJSWlrp94d/dXKFIUES8lleRoNUgO/vss2Puib/WrVvntm3+5z/+8Q8A7rnnHnesUaNGACxYsACApUuXAsG8UoBt27ZF21mJVUlJCZB8tVadOeVRUyQoIl7Lq2lzOUDTqwqTzmth0rQ5EZGqaBAUEa9pEBQRr2kQFBGvaRAUEa9pEBQRr6WbLL0TyMqqyTmqZdwdiIjOa2HSeU1BWnmCIiKFRpfDIuI1DYIi4jUNgiLiNQ2CIuI1DYIi4jUNgiLiNQ2CIuI1DYIi4jUNgiLitf8Db6qFIKi1WvcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,3)\n",
    "axs = axs.ravel()\n",
    "for i in range(len(axs)):\n",
    "    axs[i].imshow(x_train[i].reshape(28,28), cmap='gray')\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters\n",
    "\n",
    "We need to define some general parameters for networks we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128    # number of images passed each iteration\n",
    "num_classes = 10    # digits 0 to 9\n",
    "epochs = 20         # number of full passes of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "This example uses a convolutiona neural net, in this case a 2D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a CNN the data needs to be parsed as 2D array, rather than a 1D array. \n",
    "\n",
    "So, the data is reshaped according to the number of samples (60,000 or 10,000) and the size of the image 28x28.\n",
    "\n",
    "The data is then normalized (0,255) to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the content of the labels vectors looks like this:\n",
    "```\n",
    "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n",
    "```\n",
    "The NN expects them labels as binary class matrix instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the label for each image looks like this:\n",
    "```\n",
    "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# input to dense layer needs to be 1D array\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr = 0.01, momentum = 0.9, nesterov = True),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " 3328/60000 [>.............................] - ETA: 1:43 - loss: 1.9377 - acc: 0.3504"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a4b8a2636f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/michael/Data/envs/ml/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/media/michael/Data/envs/ml/local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/michael/Data/envs/ml/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/michael/Data/envs/ml/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/michael/Data/envs/ml/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

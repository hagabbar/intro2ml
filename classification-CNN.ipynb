{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "MNIST is a handwrriten digit database with a training set of 60,000 examples and a test se of 10,000 examples. The images are all 28x28 and greyscale.\n",
    "\n",
    "It is available [here](http://yann.lecun.com/exdb/mnist/), but is included in Keras and will automatically download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the 'x' arrays are the images and the 'y' arrays are the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,3)\n",
    "axs = axs.ravel()\n",
    "for i in range(len(axs)):\n",
    "    axs[i].imshow(x_train[i].reshape(28,28), cmap='gray')\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters\n",
    "\n",
    "We need to define some general parameters for networks we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128    # number of images passed each iteration\n",
    "num_classes = 10    # digits 0 to 9\n",
    "epochs = 20         # number of full passes of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "This example uses a convolutiona neural net, in this case a 2D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a CNN the data needs to be parsed as 2D array, rather than a 1D array. \n",
    "\n",
    "So, the data is reshaped according to the number of samples (60,000 or 10,000) and the size of the image 28x28.\n",
    "\n",
    "The data is then normalized (0,255) to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the content of the labels vectors looks like this:\n",
    "```\n",
    "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n",
    "```\n",
    "The NN expects them labels as binary class matrix instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the label for each image looks like this:\n",
    "```\n",
    "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# input to dense layer needs to be 1D array\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr = 0.01, momentum = 0.9, nesterov = True),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training\n",
    "This will be slow to run (~1-2 minutes per epoch) on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.5032 - acc: 0.8428 - val_loss: 0.1463 - val_acc: 0.9560\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.1959 - acc: 0.9425 - val_loss: 0.0771 - val_acc: 0.9767\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.1278 - acc: 0.9618 - val_loss: 0.0530 - val_acc: 0.9826\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.1015 - acc: 0.9695 - val_loss: 0.0445 - val_acc: 0.9854\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0829 - acc: 0.9748 - val_loss: 0.0428 - val_acc: 0.9849\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0750 - acc: 0.9774 - val_loss: 0.0381 - val_acc: 0.9885\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0653 - acc: 0.9796 - val_loss: 0.0352 - val_acc: 0.9884\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0612 - acc: 0.9815 - val_loss: 0.0334 - val_acc: 0.9891\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0539 - acc: 0.9837 - val_loss: 0.0282 - val_acc: 0.9898\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0493 - acc: 0.9851 - val_loss: 0.0289 - val_acc: 0.9905\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0476 - acc: 0.9849 - val_loss: 0.0279 - val_acc: 0.9896\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0425 - acc: 0.9864 - val_loss: 0.0295 - val_acc: 0.9905\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.0285 - val_acc: 0.9910\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 90s 2ms/step - loss: 0.0380 - acc: 0.9881 - val_loss: 0.0300 - val_acc: 0.9910\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0363 - acc: 0.9879 - val_loss: 0.0268 - val_acc: 0.9917\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0347 - acc: 0.9888 - val_loss: 0.0265 - val_acc: 0.9916\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0321 - acc: 0.9896 - val_loss: 0.0256 - val_acc: 0.9920\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0319 - acc: 0.9895 - val_loss: 0.0254 - val_acc: 0.9921\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0312 - acc: 0.9899 - val_loss: 0.0261 - val_acc: 0.9915\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0252 - val_acc: 0.9933\n",
      "Test loss: 0.025172472310285868\n",
      "Test accuracy: 0.9933\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
